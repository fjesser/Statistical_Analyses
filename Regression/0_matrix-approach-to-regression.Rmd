---
title: "Matrix Approach to Regression"
author: "Felix J. Esser"
date: "`r Sys.Date()`"
output: 
    html_document:
      toc: true
      toc_float: true
      number_sections: true
bibliography: '`r file.path(gsub("(?<=Statistical_Analyses).*", "", getwd(), perl = TRUE), "bibliography.bib")`'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This text is primarily a mathematical description of the matrix approach to multiple linear regression. It provides a deeper understanding into regression as well as linear algebra (link both). This deeper understanding is valuable in order to understand other, more complicated procedures that build up on multiple linear regression.

# Unstandardized Multiple Regression in Matrix Form

As a recap, the classical representation of multiple regression with two
predictors is $$y_i = b_0 + b_1 x_{1i} + b_2 x_{2i} + e_i$$ where $i = 1, \ldots, N$ is the
running index of the number of cases $N$ (e.g., participants), $b_0$ is the
intercept term, $b_1$ and $b_2$ are the slopes for the predictors $x_1$ and
$x_2$, respectively, and $e$ denotes the error/residual term.

More generally for $p+1$ predictors, this equation is: 
$$\begin{array}{cll} 
y_i & = &b_0 + b_1 x_{1i} + \ldots + b_j x_{ji} + \ldots + b_p x_{pi} + e_i \\
& = & b_0 + \sum_{j=1}^p b_j x_{ji} + e_i
\end{array}$$ 
where $j$ is the running index of the number of predictors. There are in total $p+1$ predictors because
there is the intercept and $p$ predictors with a slope[^1]. This equation refers
only to a single case $i$. Consequently, every case has a single equation which
makes in total $N$ equations.

[^1]: The $p+1$ predictors notation is chosen in order that the intercept is
    consistently denoted with $b_0$

All of these equations can be compactly written in matrix notation. The
representation in matrix form is then (link to Linear Algebra): 
$$\begin{array}{ccccccc}
&\mathbf{y}_{n\times 1} & = &\mathbf{X}_{n\times (p+1)} & \mathbf{b}_{p\times 1} & + & \mathbf{e}_{n\times 1}\\
=&\begin{bmatrix}y_1\\ \vdots \\ y_i \\ \vdots \\ y_n \end{bmatrix} & = & \begin{bmatrix}1 & x_{11} & \cdots & x_{j1} & \cdots & x_{p1}\\ \vdots & \vdots &  & \vdots &  & \vdots \\ 1 & x_{1n} & \cdots & x_{ji} & \cdots & x_{pi}\\ \vdots & \vdots &  & \vdots &  & \vdots \\ 1 & x_{1n} & \cdots & x_{jn} & \cdots & x_{pn} \end{bmatrix} & \begin{bmatrix}b_0\\b_1\\ \vdots \\ b_j \\ \vdots \\ b_p \end{bmatrix} & + & \begin{bmatrix}e_1\\ \vdots \\ e_i \\ \vdots \\ e_n \end{bmatrix}\end{array}$$

where $\mathbf{y}$ is the outcome vector of $n$ outcome values, $\mathbf{X}$ is the design matrix containing the constant as first column (important for intercept) and the $p$ predictor values for all $n$ cases, $\mathbf{b}$ is the coefficient vector, and $\mathbf{e}$ is the vector of errors/residuals. The equation of the predicted outcome values $\mathbf{\hat{y}}$ does not contain the residual term $\mathbf{e}$:
$$\mathbf{\hat{y}}_{n\times 1}  = \mathbf{X}_{n\times (p+1)}  \mathbf{b}_{p\times 1}$$


## Deriving $\mathbf{b}$
## Sum of Squares and ANOVA Summary Table
### Total Sum of Squares
### Regression Sum of Squares
### Residual Sum of Squares
### ANOVA Summary Table
## Standard Errors of the Regression coefficients {#se}
## Significance Tests and Confidence Intervals
# Standardized Multiple Regression in Matrix Form
# Uncorrelated predictors
# Calculation Shortcuts
