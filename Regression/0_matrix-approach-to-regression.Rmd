---
title: "Matrix Approach to Regression"
author: "Felix J. Esser"
date: "`r Sys.Date()`"
output: 
    html_document:
      toc: true
      toc_float: true
      number_sections: true
bibliography: '`r file.path(gsub("(?<=Statistical_Analyses).*", "", getwd(), perl = TRUE), "bibliography.bib")`'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This text is primarily a mathematical description of the matrix approach to multiple linear regression. It provides a deeper understanding into regression as well as linear algebra (link both). This deeper understanding is valuable in order to understand other, more complicated procedures that build up on multiple linear regression.

# Unstandardized Multiple Regression in Matrix Form

As a recap, the classical representation of multiple regression with two
predictors is $$y_i = b_0 + b_1 x_{1i} + b_2 x_{2i} + e_i$$ where $i = 1, \ldots, N$ is the
running index of the number of cases $N$ (e.g., participants), $b_0$ is the
intercept term, $b_1$ and $b_2$ are the slopes for the predictors $x_1$ and
$x_2$, respectively, and $e$ denotes the error/residual term.

More generally for $p+1$ predictors, this equation is: 
$$\begin{array}{cll} 
y_i & = &b_0 + b_1 x_{1i} + \ldots + b_j x_{ji} + \ldots + b_p x_{pi} + e_i \\
& = & b_0 + \sum_{j=1}^p b_j x_{ji} + e_i
\end{array}$$ 
where $j$ is the running index of the number of predictors. There are in total $p+1$ predictors because
there is the intercept and $p$ predictors with a slope[^1]. This equation refers
only to a single case $i$. Consequently, every case has a single equation which
makes in total $N$ equations.

[^1]: The $p+1$ predictors notation is chosen in order that the intercept is
    consistently denoted with $b_0$

All of these equations can be compactly written in matrix notation. The
representation in matrix form is then (link to Linear Algebra): 
$$\begin{array}{ccccccc}
&\mathbf{y}_{n\times 1} & = &\mathbf{X}_{n\times (p+1)} & \mathbf{b}_{p\times 1} & + & \mathbf{e}_{n\times 1}\\
=&\begin{bmatrix}y_1\\ \vdots \\ y_i \\ \vdots \\ y_n \end{bmatrix} & = & \begin{bmatrix}1 & x_{11} & \cdots & x_{j1} & \cdots & x_{p1}\\ \vdots & \vdots &  & \vdots &  & \vdots \\ 1 & x_{1n} & \cdots & x_{ji} & \cdots & x_{pi}\\ \vdots & \vdots &  & \vdots &  & \vdots \\ 1 & x_{1n} & \cdots & x_{jn} & \cdots & x_{pn} \end{bmatrix} & \begin{bmatrix}b_0\\b_1\\ \vdots \\ b_j \\ \vdots \\ b_p \end{bmatrix} & + & \begin{bmatrix}e_1\\ \vdots \\ e_i \\ \vdots \\ e_n \end{bmatrix}\end{array}$$

where $\mathbf{y}$ is the outcome vector of $n$ outcome values, $\mathbf{X}$ is the design matrix containing the constant as first column (important for intercept) and the $p$ predictor values for all $n$ cases, $\mathbf{b}$ is the coefficient vector, and $\mathbf{e}$ is the vector of errors/residuals. The equation of the predicted outcome values $\mathbf{\hat{y}}$ does not contain the residual term $\mathbf{e}$:
$$\mathbf{\hat{y}}_{n\times 1}  = \mathbf{X}_{n\times (p+1)}  \mathbf{b}_{p\times 1}$$


## Deriving $\mathbf{b}$
The regression coefficients in $\mathbf{b}$ are defined in order that the error/residual terms fullfill the least squares criterion, which is:
$$\sum_{i=1}^{n} e_i^2 = \mathbf{e'e} = min$$
This means the folllowing:
$$\begin{array}{ccl}
\mathbf{e'e} &= &(\mathbf{y} - \mathbf{\hat{y}})'(\mathbf{y} - \mathbf{\hat{y}})\\
&=& (\mathbf{y}-\mathbf{Xb})' (\mathbf{y}-\mathbf{Xb})\\
&=& \mathbf{y'y} - 2\mathbf{b'X'y} + \mathbf{b'X'Xb} = min
\end{array}$$

Differentiating $\mathbf{e'e}$ with respect to the unknown vector $\mathbf{b}$ (link to calculus or similar topic):
$$\frac{d}{d\mathbf{b}} = 2\mathbf{X'Xb}-2\mathbf{X'y}$$
Setting the obtained first derivative to 0 and solve for the desired and unknown vector $\mathbf{b}$:
$$\begin{array}{rcl}
2\mathbf{X'Xb}-2\mathbf{X'y}&=&0\\
2\mathbf{X'Xb}&=&2\mathbf{X'y}\\
\mathbf{X'Xb}&=&\mathbf{X'y}\\
\mathbf{X'X}^{-1}\mathbf{X'Xb} &= &\mathbf{X'X}^{-1}\mathbf{X'y}\\
\mathbf{b} &=& \mathbf{X'X}^{-1}\mathbf{X'y}
\end{array}$$

The last equation is really important and the heart of this document since it is the previous unknown vector of regression coefficients.

Compare this result, $\mathbf{b} = \mathbf{X'X}^{-1}\mathbf{X'y}$, with the calculation of the slope in the bivariate case and see the similarity: $b =\frac{\sum_{i=1}^{n}x_i y_i}{\sum_{i=1}^n x_i^2} = (\sum_{i=1}^n x_i^2)^{-1}\sum_{i=1}^{n}x_i y_i$



In the following the matrices involved in getting $\mathbf{b}$ are shown in more detail:

$\mathbf{X'X}_{(p+1)\times (p+1)} = \begin{bmatrix}1 & \ldots& 1\\x_{11}&\ldots& x_{1n}\\x_{21}&\ldots&x_{2n} \end{bmatrix}_{(p+1)\times n} \begin{bmatrix}1 & x_{11} & x_{21}\\ \vdots & \vdots & \vdots \\ 1 & x_{1n} & x_{2n} \end{bmatrix}_{n\times (p+1)} = \begin{bmatrix}n & \sum x_{1i} & \sum x_{2i}\\ \sum x_{1i} & \sum x_{1i}^2 & \sum x_{1i}x_{2i}\\ \sum x_{2i} & \sum x_{2i} x_{1i} & \sum x_{2i}^2 \end{bmatrix}$

$\mathbf{X'y} = \begin{bmatrix}1 & \ldots& 1\\x_{11}&\ldots& x_{1n}\\x_{21}&\ldots&x_{2n} \end{bmatrix} \begin{bmatrix}y_1\\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} \sum y_i \\ \sum x_{1i} y_i \\ \sum x_{2i} y_i \end{bmatrix}$

$\mathbf{b} = (\mathbf{X'X})^{-1} \mathbf{X'y} = \begin{bmatrix}n & \sum x_{1i} & \sum x_{2i}\\ \sum x_{1i} & \sum x_{1i}^2 & \sum x_{1i}x_{2i}\\ \sum x_{2i} & \sum x_{2i} x_{1i} & \sum x_{2i}^2 \end{bmatrix} \begin{bmatrix} \sum y_i \\ \sum x_{1i} y_i \\ \sum x_{2i} y_i \end{bmatrix} = \begin{bmatrix}n \sum y_i + \sum x_{1i} \sum x_{1i} y_i + \sum x_{2i}\sum x_{2i}y_i\\ \sum x_{1i}\sum y_i + \sum x_{1i}^2\sum x_{1i}y_i + \sum x_{1i}x_{2i}\sum x_{2i}y_i\\ \sum x_{2i}\sum y_i + \sum x_{2i}x_{1i}\sum x_{1i}y_i+ \sum x_{2i}^2\sum x_{2i}y_i \end{bmatrix} = \begin{bmatrix} b_0\\b_1\\b_2 \end{bmatrix}$



## Sum of Squares and ANOVA Summary Table
### Total Sum of Squares
### Regression Sum of Squares
### Residual Sum of Squares
### ANOVA Summary Table
## Standard Errors of the Regression coefficients {#se}
## Significance Tests and Confidence Intervals
# Standardized Multiple Regression in Matrix Form
# Uncorrelated predictors
# Calculation Shortcuts
