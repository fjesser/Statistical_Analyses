---
title: "Matrix Approach to Regression"
author: "Felix J. Esser"
date: "`r Sys.Date()`"
output: 
    html_document:
      toc: true
      toc_float: true
      number_sections: true
bibliography: '`r file.path(gsub("(?<=Statistical_Analyses).*", "", getwd(), perl = TRUE), "bibliography.bib")`'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This text is primarily a mathematical description of the matrix approach to multiple linear regression. It provides a deeper understanding into regression as well as linear algebra (link both). This deeper understanding is valuable in order to understand other, more complicated procedures that build up on multiple linear regression.

# Unstandardized Multiple Regression in Matrix Form

As a recap, the classical representation of multiple regression with two
predictors is $$y_i = b_0 + b_1 x_{1i} + b_2 x_{2i} + e_i$$ where $i = 1, \ldots, N$ is the
running index of the number of cases $N$ (e.g., participants), $b_0$ is the
intercept term, $b_1$ and $b_2$ are the slopes for the predictors $x_1$ and
$x_2$, respectively, and $e$ denotes the error/residual term.

More generally for $p+1$ predictors, this equation is: 
$$\begin{array}{cll} 
y_i & = &b_0 + b_1 x_{1i} + \ldots + b_j x_{ji} + \ldots + b_p x_{pi} + e_i \\
& = & b_0 + \sum_{j=1}^p b_j x_{ji} + e_i
\end{array}$$ 
where $j$ is the running index of the number of predictors. There are in total $p+1$ predictors because
there is the intercept and $p$ predictors with a slope[^1]. This equation refers
only to a single case $i$. Consequently, every case has a single equation which
makes in total $N$ equations.

[^1]: The $p+1$ predictors notation is chosen in order that the intercept is
    consistently denoted with $b_0$

All of these equations can be compactly written in matrix notation. The
representation in matrix form is then (link to Linear Algebra): 
$$\begin{array}{ccccccc}
&\mathbf{y}_{n\times 1} & = &\mathbf{X}_{n\times (p+1)} & \mathbf{b}_{(p+1)\times 1} & + & \mathbf{e}_{n\times 1}\\
=&\begin{bmatrix}y_1\\ \vdots \\ y_i \\ \vdots \\ y_n \end{bmatrix} & = & \begin{bmatrix}1 & x_{11} & \cdots & x_{j1} & \cdots & x_{p1}\\ \vdots & \vdots &  & \vdots &  & \vdots \\ 1 & x_{1n} & \cdots & x_{ji} & \cdots & x_{pi}\\ \vdots & \vdots &  & \vdots &  & \vdots \\ 1 & x_{1n} & \cdots & x_{jn} & \cdots & x_{pn} \end{bmatrix} & \begin{bmatrix}b_0\\b_1\\ \vdots \\ b_j \\ \vdots \\ b_p \end{bmatrix} & + & \begin{bmatrix}e_1\\ \vdots \\ e_i \\ \vdots \\ e_n \end{bmatrix}\end{array}$$

where $\mathbf{y}$ is the outcome vector of $n$ outcome values, $\mathbf{X}$ is the design matrix containing the constant as first column (important for intercept) and the $p$ predictor values for all $n$ cases, $\mathbf{b}$ is the coefficient vector, and $\mathbf{e}$ is the vector of errors/residuals. The equation of the predicted outcome values $\mathbf{\hat{y}}$ does not contain the residual term $\mathbf{e}$:
$$\mathbf{\hat{y}}_{n\times 1}  = \mathbf{X}_{n\times (p+1)}  \mathbf{b}_{(p+1)\times 1}$$


## Deriving $\mathbf{b}$
The regression coefficients in $\mathbf{b}$ are defined in order that the error/residual terms fullfill the least squares criterion, which is:
$$\sum_{i=1}^{n} e_i^2 = \mathbf{e'e} = min$$
This means the folllowing:
$$\begin{array}{ccl}
\mathbf{e'e} &= &(\mathbf{y} - \mathbf{\hat{y}})'(\mathbf{y} - \mathbf{\hat{y}})\\
&=& (\mathbf{y}-\mathbf{Xb})' (\mathbf{y}-\mathbf{Xb})\\
&=& \mathbf{y'y} - 2\mathbf{b'X'y} + \mathbf{b'X'Xb} = min
\end{array}$$

Differentiating $\mathbf{e'e}$ with respect to the unknown vector $\mathbf{b}$ (link to calculus or similar topic):
$$\frac{d \mathbf{e'e}}{d\mathbf{b}} = 2\mathbf{X'Xb}-2\mathbf{X'y}$$
Setting the obtained first derivative to 0 and solve for the desired and unknown vector $\mathbf{b}$:
$$\begin{array}{rcl}
2\mathbf{X'Xb}-2\mathbf{X'y}&=&0\\
2\mathbf{X'Xb}&=&2\mathbf{X'y}\\
\mathbf{X'Xb}&=&\mathbf{X'y}\\
(\mathbf{X'X})^{-1}\mathbf{X'Xb} &= &(\mathbf{X'X})^{-1}\mathbf{X'y}\\
\mathbf{b} &=& (\mathbf{X'X})^{-1}\mathbf{X'y}
\end{array}$$

The last equation is really important and the heart of this document since it is the previous unknown vector of regression coefficients.

Compare this result, $\mathbf{b} = (\mathbf{X'X})^{-1}\mathbf{X'y}$, with the calculation of the slope in the bivariate case and see the similarity: $b =\frac{\sum_{i=1}^{n}x_i y_i}{\sum_{i=1}^n x_i^2} = (\sum_{i=1}^n x_i^2)^{-1}\sum_{i=1}^{n}x_i y_i$



In the following the matrices involved in getting $\mathbf{b}$ are shown in more detail:

$\mathbf{X'X}_{(p+1)\times (p+1)} = \begin{bmatrix}1 & \ldots& 1\\x_{11}&\ldots& x_{1n}\\x_{21}&\ldots&x_{2n} \end{bmatrix}_{(p+1)\times n} \begin{bmatrix}1 & x_{11} & x_{21}\\ \vdots & \vdots & \vdots \\ 1 & x_{1n} & x_{2n} \end{bmatrix}_{n\times (p+1)} = \begin{bmatrix}n & \sum x_{1i} & \sum x_{2i}\\ \sum x_{1i} & \sum x_{1i}^2 & \sum x_{1i}x_{2i}\\ \sum x_{2i} & \sum x_{2i} x_{1i} & \sum x_{2i}^2 \end{bmatrix}$

$\mathbf{X'y} = \begin{bmatrix}1 & \ldots& 1\\x_{11}&\ldots& x_{1n}\\x_{21}&\ldots&x_{2n} \end{bmatrix} \begin{bmatrix}y_1\\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} \sum y_i \\ \sum x_{1i} y_i \\ \sum x_{2i} y_i \end{bmatrix}$

$\mathbf{b} = (\mathbf{X'X})^{-1} \mathbf{X'y} = \begin{bmatrix}n & \sum x_{1i} & \sum x_{2i}\\ \sum x_{1i} & \sum x_{1i}^2 & \sum x_{1i}x_{2i}\\ \sum x_{2i} & \sum x_{2i} x_{1i} & \sum x_{2i}^2 \end{bmatrix} \begin{bmatrix} \sum y_i \\ \sum x_{1i} y_i \\ \sum x_{2i} y_i \end{bmatrix} = \begin{bmatrix}n \sum y_i + \sum x_{1i} \sum x_{1i} y_i + \sum x_{2i}\sum x_{2i}y_i\\ \sum x_{1i}\sum y_i + \sum x_{1i}^2\sum x_{1i}y_i + \sum x_{1i}x_{2i}\sum x_{2i}y_i\\ \sum x_{2i}\sum y_i + \sum x_{2i}x_{1i}\sum x_{1i}y_i+ \sum x_{2i}^2\sum x_{2i}y_i \end{bmatrix} = \begin{bmatrix} b_0\\b_1\\b_2 \end{bmatrix}$



## Sum of Squares and ANOVA Summary Table

The total sum of squares can be decomposed into the regression/model sum of squares and the error/residual sum of squares:
$$SS_{Total} = SS_{Regression} + SS_{Residual}$$
It is important to get the residual sum of squares and their degree of freedoms in order to estimate the [standard errors of the regression coefficients](#se). In addition it is possible to create an ANOVA summary table based on the sum of squares.

Add
- MS = SS/df
- dfs also sum up
- but MS don't sum up


### Total Sum of Squares
The total sum of squares represent the variability in $\mathbf{y}$ that is available:
$$\begin{array}{rcl}
SS_{Total} &= &\mathbf{y_c'y_c} = \mathbf{(y - \bar{y})' (y - \bar{y})} = \begin{bmatrix}y_{c1} & \ldots & y_{cn} \end{bmatrix} \begin{bmatrix}y_{c1} \\ \vdots \\ y_{cn}\end{bmatrix}\\
&= &\sum_{i=1}^{n} (y_i - \bar{y})^2 =  \sum_{i=1}^{N} y_i^2 -n\cdot \bar{y}^2 
\end{array}$$
The corresponding degrees of freedom of the total sum of squares are:
$$df_{Total} = n -1$$

Dividing the total sum of squares by the total degrees of freedom results in the total mean squares. The total mean squares are equal to the variance of $\mathbf{y}$:
$$MS_{Total} = \frac{SS_{Total}}{df_{Total}} = \frac{\sum_{i=1}^{n} (y_i - \bar{y})^2}{n-1} = \hat{\sigma}_y^2$$

### Regression Sum of Squares
The regression sum of squares represent the variability that could be explained by the linear regression. The regression sum of squares are based on the fitted values $\mathbf{\hat{y}}$:
$$\mathbf{\hat{y}} = \mathbf{X b}$$
Before using these fitted values, they must be centered so that the variability is correct. This is the same as in the calculation of the total sum of squares where the mean is subtracted from every value before squaring them and summing them up. The centered fitted values are:
$$\mathbf{\hat{y}_C} = \mathbf{\hat{y}} - \mathbf{\bar{y}} = \begin{bmatrix}\hat{y}_1 \\ \vdots \\ \hat{y}_n \end{bmatrix} \begin{bmatrix}\bar{y}\\ \vdots \\ \bar{y} \end{bmatrix} = \begin{bmatrix}\hat{y}_{c1} \\ \vdots \\ \hat{y}_{cn} \end{bmatrix}$$
Now, the regression sum of squares can be calculated:
$$\begin{array}{rcl}
SS_{Regression} &= &SS_{\mathbf{\hat{y}_C}} \\
& = & \mathbf{\hat{y}_C' \hat{y}_C} = \begin{bmatrix}\hat{y}_{c1} & \ldots & \hat{y}_{cn} \end{bmatrix} \begin{bmatrix}\hat{y}_{c1} \\ \vdots \\ \hat{y}_{cn}\end{bmatrix}\\
& = & \sum_{i=1}^{n} y_{ci}^2 = \sum_{i=1}^{n} (\hat{y}_i-\bar{y})^2
\end{array}$$
The regression sum of squares are the sum of squares of the (centered) predicted values ($\mathbf{\hat{y}}$)


The corresponding degrees of freedom are the number of predictors (regression coefficients that represent slopes):
$$df_{Regression} = p$$

The regression mean squares are:
$$MS_{Regression} = \frac{SS_{Regression}}{df_{Regression}} = \frac{\sum_{i=1}^{n} (\hat{y}_i-\bar{y})^2}{p}$$


### Residual Sum of Squares
The residual sum of squares represent the variability that could not be explained. First, the residual vector has to be calculated:
$$\mathbf{e} = \mathbf{Y} - \mathbf{\hat{Y}} = \begin{bmatrix}y_1 \\ \vdots \\ y_n \end{bmatrix} - \begin{bmatrix} \hat{y}_1 \\ \vdots \\ \hat{y}_n \end{bmatrix} = \begin{bmatrix}e_1 \\ \vdots \\ e_n \end{bmatrix}$$

Based on the residual vector the residual sum of squares can be calculated:
$$SS_{Residual} = \mathbf{e'e} = \begin{bmatrix}e_1 & \ldots & e_n \end{bmatrix} \begin{bmatrix}e_1 \\ \vdots \\ e_n \end{bmatrix} = \sum_{i=1}^n e_i^2$$

The corresponding degrees of freedom are:
$$df_{Residual} = n-p-1$$

The mean residual squares which is also called the conditional variance and mean squared error:
$$MS_{Residual} = \frac{SS_{Residual}}{df_{Residual}} = \frac{\sum_{i=1}^n e_i^2}{n-p-1} = MSE = \hat{\sigma}_e^2$$
The conditional variance is an estimate for the variance of the residuals around the regression line. 

### ANOVA Summary Table and $R^2$

: ANOVA Summary Table

| Source        | SS               | df               | MS              | F              | p        |
|---------------|:----------------:|:----------------:|:---------------:|:--------------:|:--------:|
| **Regression**| $SS_{Regression}$| $df_{Regression}$|$MS_{Regression}$|$F= \frac{MS_{Regression}}{MS_{Residual}}$|  |
| **Residual**  | $SS_{Residual}$  | $df_{Residual}$  |$MS_{Residual}$  |                |          |
| **Total**     | $SS_{Total}$     | $df_{Total}$     |                 |                |          |


Necessary to report also $R^2$

$$R^2 = \frac{SS_{Regression}}{SS_{Total}}$$


## Standard Errors of the Regression coefficients {#se}

Add how to come to the next equation:

Using the mean squared error, $\hat{\sigma}_e^2 = MS_{Residual} = \frac{SS_{Residual}}{df_{Residual}}$, it is possible to derive the covariance matrix of the coefficients:
$$Var(\mathbf{b})_{(p+1)\times (p+1)} = 
\hat{\sigma}^2_e \mathbf{(X'X)}^{-1} = 
\begin{bmatrix}\sigma^2_{b0}&\cdots &&&\\ \vdots & \ddots & & &\\ \vdots & \cdots & \sigma^2_{bj} & \cdots & \vdots \\ \vdots & & & \ddots & \vdots \\ \cdots & \cdots & \cdots & \cdots & \sigma^2_{bp} \end{bmatrix}$$

Since the mean squared error is only one value, the assumption of homoskedasticity is necessary to provide the same accuracy of a fitted value for every predictor combination. 

Add that in the [standardized regression](#std-reg) the diagonal elements of $Var(\mathbf{b})$ are the variance inflation factors.

The diagonal elements are the squared standard error of the coefficients.  Therefore, the standard error of the regression coefficient for the intercept is the square root of the first element (first row, first column) of the $Var(\mathbf{b})$ matrix:
$$SE(b_0) = \sqrt{Var(\mathbf{b})_{11}}$$
Similarly, the square root of the element in the second row and second column of $Var(\mathbf{b})$ is the standard error of the regression coefficient of the first predictor:
$$SE(b_1) = \sqrt{Var(\mathbf{b})_{22}}$$

In general, taking the square root of all diagonal elements results in a vector of standard errors:
$$SE(\mathbf{b}) = \sqrt{diag(Var(\mathbf{b}))}$$


## Significance Tests and Confidence Intervals
# Standardized Multiple Regression in Matrix Form
# Uncorrelated predictors
# Calculation Shortcuts
