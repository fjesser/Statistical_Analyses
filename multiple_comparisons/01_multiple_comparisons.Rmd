---
title: 
author: "Felix Eßer"
date: "`r Sys.Date()`"
output: 
    html_document:
      toc: true
      toc_float: true
      number_sections: true
bibliography: '`r file.path(gsub("(?<=Statistical_Analyses).*", "", getwd(), perl = TRUE), "bibliography.bib")`'
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Multiple Testing refers to the situation in which one conducts several statistical inferences.
It is of interest that the null hypothesis can be safely rejected which means that no $\alpha$-error is made. Therefore, the $\alpha$-error rate is conventionally set to $\alpha = .05$. However, when conducting INDEPENDENT multiple tests


A priori (also planned comparison) vs. a posteriori (also post-hoc)


Terminology:

- alpha
    - false discovery
- family-wise error rate (FWER)
    + also called experimentwise error rate
- false discovery rate (FDR)
- Multiple Testing procedures (adjusting p-values)
    - One-Step
        - all $p$ values are compared to a predetermined cut-off
    - Step-down
        - p values are evaluated from smallest to largest
        - each p value is compared to its own cut-off
        - if a p-value is not significant, the null-hypothesis is accepted for the corresponding test as well as all remaining tests having higher p-values (even if higher p value would be sig. )
    - Step-Up




family-wise error rate ($FWER$; also called experiment-wise error rate) is the probability of making at least one $\alpha$-error (type I error) in a set of statistical tests:
$$ FWER = \alpha_{fam} = 1 - (1-\alpha_r)^m$$
Here, $\alpha_r$ is the the $\alpha$-level for a specific test $r$ and $s$ denotes the number of all tests (see appendix for derivation).

When there are $g$ means, there are:
$$s = \frac{g \cdot (g-1)}{2}$$


```{r packages, message=FALSE}
library(tidyverse)
```

```{r fwer}
# vector m representing total number of tests
m <- 1:50
# alpha represents alpha for one test
alpha <- 0.05
# family-wise error-rate (fwer) dependent on m and alpha
fwer <- 1 - (1 - alpha)^m


ggplot(mapping = aes(x = m, y = fwer)) +
  geom_point(size = 1) +
  scale_x_continuous(breaks = seq(0, 50, 5)) +
  scale_y_continuous(breaks = seq(0, 0.90, 0.20)) +
  labs(title = bquote(
    atop("Family-Wise Error Rate ("*italic("FWER")*") Dependent on the Number of Total Tests ("*italic("m")*")",
         "with"~alpha == .(alpha))),
    x = expression(paste("Number of total tests (", italic("m"), ")")),
    y = expression(paste("Family-Wise Error Rate (", italic("FWER"), ")"))) +
  theme_minimal()


```

For $m = 20$ tests, the probability to have at least one false-significant result (with a significance level $\alpha = .05$ for each test) is $ FWER = \alpha_{fam} = 1 - (1-.05)^{20} = 0.64$. This means there is a 64% chance of getting at least one false-significant result. With $m = 20$ test, one would expect to have $m \cdot \alpha = 20 \cdot 0.05 = 1$ falsely rejected null-hypothesis (also called false discovery). As the figure shows, $FWER$ increases with the number of tests ($m$).


- What and why
    + Also knwon as 
        - multiple comparison (but closer to ANOVA and mean differences)
    + Multiple Testing refer to the situation where one is interested in 
    + $\alpha$ error inflation (link to alpha error)
        + what is a p value (short definition and link)
        + $p$-value is defined as the probability of obtaining a test statistic at least as large as the one observed, if the null hypothesis is true
    + family-wise error-ate and false discovery rate
- Where/Contexts
    + ANOVA 
    + correlations
    + but applicable anywhere
- How 
    + p-Value Adjustment
        - more independent from statistical analysis
        - applicable everywhere, where $p$ values are
    + Resampling Methods

    
# Individual $t$-tests

is a special case of linear contrast (link to linear contrast; look where to place this maybe)


Form of a priori comparisons 
usually considered a terrible approach, however under special circumstances this might be okay (small number of comparisons that are really preplanned)
no real adjustment is made 

If homogeneity of variances is given and sample sizes are the same:
Replace variances in formula with mean squared error (MSE)

$$t = \frac{\bar{Y}_i - \bar{Y}_j}{\sqrt{\frac{MS_{Error}}{n} + \frac{MS_{Error}}{n}}} = \frac{\bar{Y}_i - \bar{Y}_j}{\sqrt{\frac{2\cdot MS_{Error}}{n}}}$$
Evaluate $t$ with $df_{Error}$


If variances are heterogeneous and sample sizes are equal:
- use individual group variances and evaluate $t$ on $df = 2(n-1)$


If variances are heterogeneous and sample sizes are unequal:
- use individual variances and correct degrees of freedom using Welch-Satterwaite approach (link to t-test)

    
# Adjustment of $\alpha$

## Bonferroni adjustment

- divide the $\alpha$-level by the $m$ tests to get the new $\alpha$ level for all $r$ tests

Advantages:
- easy 
Disadvantages:
- very conservative
    + for many tests 


$$\alpha_r = \frac{\alpha_{fam}}{s}$$


$r$ = specific test 

$s$ = total number of comparisons



## Bonferroni-Holm adjustment

Steps in the Bonferroni-Holm adjustment
1. Rank comparisons from most significant to least significant
2. For each comparison get $\alpha_r$

$$\alpha_r = \frac{\alpha_{fam}}{s - (r-1)}$$
$r$ = specific test, $s$ = total number of comparisons




## Šidák adjustment

- less conservative than Bonferroni


$$\alpha_r = \sqrt[s]{1- \alpha_{fam}}$$

$r$ = specific test 

$s$ = total number of comparisons

# Adjustment of critical values in mean comparisons 
- only relevant for mean comparisons (ANOVA)

- maybe decide to include it elsewhere (Comparing Means)
    + at least make a link
    
    
see also Kirk 2012    
    
    
classification:
- studentized range based test (e.g., Tukey but also a lot of others)
    
    
## Tukey HSD and Tukey-Kramer
  - TukeyHSD only for equal sample sizes
  - Tukey Kramer for unequal sample sizes

comparison of all means

use of studentized range distribution (link to distributions): $q$ statistic


Standard error in original Tukey's HSD for equal sample sizes
$$se_{\bar{Y}_j - \bar{Y}_{j'}} = \sqrt{\frac{MS_{Residual}}{n}}$$
where $n$ is the group size and $n_j = n_{j'}$


Standard error in Tukey-Kramer HSD for unequal sample sizes


There are two possibilities to format the 


Often, the standard error for the Tukey-Kramer test is stated like this:
$$se_{q} = \sqrt{\frac{MS_{Residual}}{2} \left(\frac{1}{n_j} + \frac{1}{n_{j'}} \right)}$$
which is the standard error of the q statistic

The associated critical value is a studentized range statistic $q$. This is dependent on the number of means (groups) and $df_{Residual}$

```{r}
# get critical studentized range q
qtukey(1-0.05, nmeans = 5, df = 10) # abitrary values
```



However, it is better to rewrite the standard error and extract $\sqrt{\frac{1}{2}}$, which will result in the standard error in means
$$se_{\bar{Y}_j - \bar{Y}_{j'}} = \sqrt{MS_{Residual} \left(\frac{1}{n_j} + \frac{1}{n_{j'}} \right)}$$
This leads to a the following different critical value:
$$t_{df_{Error}}^* = \frac{q_{g;df_{Error}}}{\sqrt{2}}$$


With both, versions of standard errors and corresponding critical values, the following can be done:
$$95\%~CI = (\bar{Y}_j - \bar{Y}_j') \pm cv\cdot se$$
cv = critical value



```{r}

library(emmeans)
dat <- datasets::ToothGrowth |> 
  mutate(dose = factor(dose))

aov_mod <- aov(len ~ dose, data = dat)
summary(aov_mod)

TukeyHSD(aov_mod)

pairs(emmeans(aov_mod, ~ dose), adjust = "tukey") |> str()
# --> no CI

library(multcomp)

summary(glht(aov_mod, linfct = mcp(dose = "Tukey"), alternative = "greater"))

```


## Games and Howell Procedure

is Tukey HSD for unequal sample sizes and unequal variances!

## Dunnett Test



# Benjamini-Hochberg Test

focuses on false-discovery rate and not familywise error rate.

Steps
1. Rank the $s$ unadjusted $p$ values in ascending order (from most to least significant) with the rank $i$
2. Calculate new $\alpha_r = \left(\frac{i}{s} \right) \cdot \alpha_{fam}$ for every single comparison $r$
3. Determine whether the original/unadjusted $p$ value is smaller than the new $\alpha_r$, then it is significant

1. Rank comparisons from most significant to least significant
2. Calculate new $\alpha_r = \frac{i}{s}\cdot \alpha_{fam}$
   - $i$ = rank
   - $s$ = number of comparisons ($s =\frac{j*(j-1)}{2}$)
3. Determine whether the original $p$ is smaller than $\alpha_r$, then it is significant


Include a table to clarify this



```{r}
pairwise.t.test(data_indep1$weight, data_indep1$group, 
                p.adjust.method = "BH") # BH = Benjamini-Hochberg
```

in order to get adjuster $p$ values:

$p < \left(\frac{i}{m}\right) \cdot \alpha_{fam}$

to 

$p \cdot \left(\frac{m}{i}\right) <  \alpha_{fam}$



# Appendix

The formula for the family-wise error rate ($FWER$) can be derived from the multiplication theorem of probabilites (link to probability):
For two independent evens, event $A$ and event $B$, the probability that both event 
$$P(A \cap B) = P(A) \cdot P(B)$$

# References
