---
title: "Comparing Means - ANOVA"
author: "Felix Eßer"
date: "10/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Analysis of Variance (ANOVA) is a family of analyses to compare the mean of several samples. Hence, the outcome variable has at least to be metric scale of measurement and the predictor variable(s) has/have nominal scale of measurement.

In the context of ANOVA, the predictor is often called factor or independent variable (note that independent variable implies causal influence). Factors have levels which are also called conditions. For example, in a clinical study it is of interest how a new drug influences the blood pressure of patients. To assess the efficacy of the new drug it is compared to an old drug as well as a placebo. The factor *drug* in this study has three levels: 1) new drug, 2) old drug and 3) placebo. After assigning participants randomly to one of the three levels, administering the corresponding drug and measure the blood pressure, it is possible to compare the means of all blood pressures in the three conditions with a ANOVA. 


One-way ANOVA with factor of 3 levels.

Another factor gender can be added. Then a two-way ANOVA with a 3x2 design (3 levels of factor drug and 2 levels of factor gender)

Different kinds of designs: 
- 1 factor: one-way ANOVA 
- 2 factors: two-way ANOVA 
- and so on

It is also possible that a factor comprises dependent samples. A typical reason are repeated measures of the same cases.

$F$-test is an omnibus test (global test) Follow up with contrast analysis or post-hoc tests

The different kinds of ANOVA are presented in the following way: 
1. Comparing several independent samples 
        + One-way ANOVA 
            e.g., the aforementioned example of the influence of different drugs on blood pressure
        + Two-way ANOVA 
            e.g., th
2. Comparing several dependent samples 
        + One-way ANOVA for dependent samples
        + Two-way ANOVA for dependent samples
3. Comparing independent and dependent samples

fixed and random effects 
+ fixed effects - factor has fixed values - means of the factor levels are of interest + random effects - factor levels are a random draw of several possible manifestations of the factor - variance of the means of the factor levels is of interest + distinction has no consquence for one-way ANOVA but for two-way ANOVA



$\alpha$-error cumulation

Two approaches as follow-up analysis after a significant $F$-test: 1. Post-hoc tests 2. Contrasts analysis

## Post-hoc tests

Post-hoc tests compare means of different factor levels.

To avoid $\alpha$-error cumulation, there are two common procedures: 
1. Adjustment of the $\alpha$-error of specific comparisons 
        + typically used if only selected mean comparisons are of interest
2. Adjustment of the critical values for multiple comparisons
        + typically used if all possible comparisons are of interest

There are a lot of procedures for post-hoc test.

### Adjustment of the $\alpha$-error of specific comparisons

1.  Sidak Adjustment
2.  Bonferroni Adjustment
3.  Bonferroni-Holm Adjustment

### Adjustment of the critical values for multiple comparisons

1.  Tukey-Test and Tukey-Kramer-Test Tukey-test is also known as HSD-test: Honest-Significant-Difference-test. Tukey-test is requires balanced sample sizes, whereas the Tukey-Kramer-test allows for differing sample sizes.

Has more power than bonferroni-correction if all means are compared with each other.

2.  Dunnett-Tes

## Contrast analysis

+-----------------------+---------------------------------------------------------------+------+
| Name of contrasts     | Explanation                                                   | Col3 |
|                       |                                                               |      |
| (R Code)              |                                                               |      |
+=======================+===============================================================+======+
| Dummy (default)       | Each factor level is compared to the first factor level.      |      |
+-----------------------+---------------------------------------------------------------+------+
| SAS                   | Each factor level is compared to the last factor level.       |      |
|                       |                                                               |      |
| (`contr.SAS()`)       |                                                               |      |
+-----------------------+---------------------------------------------------------------+------+
| Treatment             | Each factor level is compared to a user defined factor level. |      |
|                       |                                                               |      |
| (`contr.treatment()`) |                                                               |      |
+-----------------------+---------------------------------------------------------------+------+
| Sum                   |                                                               |      |
|                       |                                                               |      |
| (`contr.sum()`)       |                                                               |      |
+-----------------------+---------------------------------------------------------------+------+
| Helmert               |                                                               |      |
|                       |                                                               |      |
| (`contr.helmert()`)   |                                                               |      |
+-----------------------+---------------------------------------------------------------+------+
| Poly                  |                                                               |      |
|                       |                                                               |      |
| (`contr.poly()`)      |                                                               |      |
+-----------------------+---------------------------------------------------------------+------+

: Predefined Contrasts in \`R\`


### $\alpha$-error cumulation

Scheffé test

# Packages

```{r packages}
library(afex) # ANOVA
library(car) # Levene Test
library(effectsize) # omega^2
library(emmeans) # estimate marginal means
library(multcomp) # control for alpha error in multiple testing
library(skimr) # descriptive statistics
library(tidyverse) # data wrangling and plotting
```

# Comparing several independent samples

## One-way ANOVA

### Theory of one-Way ANOVA

The one-way ANOVA is a extension of the $t$-test.

Only an omnibus test:

Null-hypothesis: $H_0: \mu_i = \mu_j$ for all pairs \$(i, j),\~\~i \neq j \$ Alternative hypothesis: $H_1: \neg H_0$

This hypothesis is tested via a $F$-test (always one-sided). If the null-hypothesis has to be rejected, it can only be derived **that** means differ but not which.

Effect Size $\hat{\eta}^2 = \frac{SS_B}{SS_T}$ (see appendix for explanation). However, this

#### Assumptions of one-way ANOVA

1.  Independent residuals + residuals are independent from another + this assumption is fulfilled via the research design + is for example violated through repeated measures + violation increases hugely probability of committing an $\alpha$ error

2.  Homoskedasticity + Equal variance of residuals between conditions + is tested with Levene's test for equal variances + The $F$-test is relatively robust against violations of the assumption of homoskedasticity if samples have the same size - if variance in smaller sample is larger than in a larger sample, the $F$-test becomes too liberal (higher risk of committing an $\alpha$ error) - if variance in smaller sample is smaller than in a larger sample, the $F$-test becomes too conservative (higher risk of committing an $\beta$ error) + If there is variance heterogeneity (heteroskedasticity), the Brown-Forsythe-test or the Welch-test can be used which correct the result. No one of both is superior to the other.

3.  Normal distribution of residuals\

    -   As a residuum is the deviation of a measurement from the mean of the condition, measurements have to distribute normally within conditions + On the basis of the central limit theorem, the $F$-test is relatively robust against violations against the normality assumption under large sample sizes. So you can argue this assumption away. + If the violation of the normality assumption is due to outliers, robust variants of ANOVA can be conducted.

### Data

```{r indep1-data}
# load data
data_indep1 <- datasets::PlantGrowth %>% 
    as_tibble() %>% # convert data.frame to tibble
    rowid_to_column("ID") # adding ID variable

# get descriptives
skim(data_indep1)
```

In the dataset `data_indep1` the weight of plants is measured under three different conditions. Each condition comprises ten plants.

```{r indep1-descr}
# Get descriptive statistics per group
data_indep1 %>% 
    select(-ID) %>% # discard ID variable
    group_by(group) %>% # group output by grouping variable 'group'
    skim() # get descriptive statistics
```

As shown in the output the mean weight in the control group is `r mean(filter(data_indep1, group == "ctrl")$weight)`, in the first treatment group `r mean(filter(data_indep1, group == "trt1")$weight)` and in the second treatment group `r mean(filter(data_indep1, group == "trt2")$weight)`. The question which is possible to answer with the one-way ANOVA is, whether these means differ significantly or differ by chance.

### Assessing assumptions of one-way ANOVA

1.  Independent residuals + residuals are independent from another + this assumption is fulfilled via the research design + is for example violated through repeated measures
2.  Homoskedasticity

```{r indep1-levene-test}
leveneTest(weight ~ group, data = data_indep1)
```

3.  Normal distribution of residuals\

    -   As a residuum is the deviation of a measurement from the mean of the condition, measurements have to distribute normally within conditions
    
```{r indep1-shapiro-test}
# Calculate shapiro wilk test for each group
tapply(data_indep1$weight, data_indep1$group, shapiro.test)
```
    

### One-way ANOVA analysis with `{stats}`

One-way ANOVA can be conducted with the `aov()` function of the `{stats}` package.

```{r indep1-anova-aov}
# Perform one-way ANOVA
anova_aov <- aov(weight ~ group, data = data_indep1)

# Get results
summary(anova_aov) # same as: anova(anova_aov)
```

Note that the ANOVA can be reframed as a linear model. In this case the three factor level will be coded into two dummy variables (number of dummy variables = $k -1$, where $k$ is the number of levels). In this way the one-way ANOVA can be computed via the `lm()` function of the `{stats}` package.

```{r indep1-anova-lm}
# Perform one-way ANOVA with lm()
anova_lm <- lm(weight ~ group, data = data_indep1)

# Get results
summary(anova_lm)
```

The estimate of the intercept represents the mean of the reference group (here control group). The estimates of the other coefficients are the respective difference of the corresponding group to the reference group.

Both, the `aov()` and the `lm()` function, calculate the same model but the results are presented differently.

It is possible to transform the `aov()` output into the `lm()` output and vice versa.

```{r indep1-anova-aov-to-lm}
# Transform aov() output into lm() output
summary.lm(anova_aov)

# Transform lm() output into aov() output
summary.aov(anova_lm) # same as anova(anova_lm)

```

A better function is the `oneway.test()` function of the `{stats}` package because it corrects for unequal variances.

```{r indep1-anova-oneway}
# Performing one-way ANOVA without assuming equal variances
oneway.test(weight ~ group, data = data_indep1)
```

### One-way ANOVA analysis with `{afex}`

```{r indep1-anova-afex}
# Perform one-way ANOVA with afex-package
anova_afex <- aov_ez(id = "ID", dv = "weight", between = "group",
                     data = data_indep1)

# Get results
summary(anova_afex)
```

### Contrasts

#### Contrasts with `{stats}`

Set contrasts as attribute to the factor in the `data.frame`

```{r}
contrasts(data_indep1$group) <-  contr.helmert(3) # 3 groups / levels of factor
```

### Post-hoc Tests

#### Adjustment of the $\alpha$-error of specific comparisons

```{r}
pairwise.t.test(data_indep1$weight, data_indep1$group,
                p.adjust.method = "bonferroni",
                alternative = "two.sided") # default: two.sided 
```

Available $p$-value adjustments in the base `R` `{stats}` package: 1. holm 2. hochberg 3. hommel 4. bonferroni 5. BH (Benjamini & Hochberg) --\> controls for false discovery rate 6. fdr = same as BH 7. Benjamini, Hochberg, and Yekutieli --\> controls for false discovery rate

#### Adjustment of the critical values for multiple comparisons

Use `TukeyHSD()` function from `{stats}` package

```{r}
TukeyHSD(anova_aov)
```

### Report

Add ANOVA table (SS, df, MS, F, p, eta\^2 or omega\^2) Often the row with the total variation is ommited because it can be derived from the other sources of variance.

Report in text:

Visualization with bar diagram with error bars (either sd, se or ci)

## Two-Way ANOVA

### Theory of two-way ANOVA

main effects and interaction effects

simple effects
- 


marginal means 


# Comparing several dependent samples

## One-way ANOVA for dependent samples

### Theory of one-way ANOVA for dependent samples


### Data

### Assumptions of ANOVA for several dependent groups

#### Testing the Assumptions of ANOVA for several dependent groups


### Dependent Samples ANOVA with `{stats}`

### Dependent Samples ANOVA with `{afex}`


### Effect Sizes

### Contrasts

### Post hoc tests

### Visualizing the results

## Two-way ANOVA for dependent samples


### Post-hoc tests

# Comparing independent and dependent samples

```{r}
datasets::sleep
```

## Assumptions


# Appendix: Theory and Mathematics of ANOVA

## Comparing several independent samples

### One-way ANOVA

### Two-way ANOVA

## Comparing several dependent samples

## Comparing independent and dependent samples
