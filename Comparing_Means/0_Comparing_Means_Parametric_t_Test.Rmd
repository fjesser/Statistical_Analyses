---
title: "Comparing Means - t-Test"
author: "Felix EÃŸer"
date: "10/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction


Comparing two means.

Effect sizes:
- cohen's $d$ is positively biased, hedges' $g$ is a bias corrected version for small sample sizes.




# Packages

```{r packages}
library(effectsize) # for effect size
library(skimr) # descriptive statistics
library(tidyverse) # data wrangling, plotting, etc.
```


# Comparing an observed mean to an expected mean - one sample $t$-test

# Comparing two independent groups - independent two sample $t$-tests
# Comparing two dependent groups - dependent two sample $t$-test

The means on a metric outcome of two dependent samples can be compared with the $t$-test for dependent samples. Dependent samples can arise through repeated measurement of the same cases. As long as one measurement of one sample can be connected to a measurement of the other sample, dependent samples are present. Therefore, spouses are also an example of dependent samples.

If the comparison of more than two dependent samples are of interest, the ANOVA for dependent samples can be used.

At first the dataset will be created.

```{r dependent-data}
# Load dat
dep_data <- data.frame(
  pre = c(9, 8, 1, 3, 2),
  post = c(16, 11, 15, 12, 9)
) %>% as_tibble()

# Show data
dep_data
```

Now, we get the descriptive statistics for the two dependent samples, as well as the difference. These will be incorporated into the report of the $t$-test for dependent samples (below). The mean and standard deviation are outputted along with other statistics by aid of the `skim()` function of the `{skimr}` package.

```{r descriptive-dep}
# Calculate descriptive statistics 
dep_descr <- 
  dep_data %>% 
  mutate(difference = post - pre) %>% # create difference variable
  skim()

# Output descriptive statistics
dep_descr
```



## Assumptions of the dependent $t$-test

The $t$-test for dependent samples has the following two assumptions:
1. Metric scale of measurement of the outcome and two dependent samples as predictor
2. Independent observations *within* the dependent samples
    - The cases are not allowed to influence on each other
    - This assumption is fulfilled through the research design
3. Normal distribution of difference scores
    - The difference variable (here: difference = post scores - pre scores) has to distribute normally. 
    - This assumption can be tested with the Shapiro-Wilk test
    - However, the $t$-test is relatively robust against violations and furthermore with large sample sizes the central limit theorem applies. 
    - look at skew and symmetry (elaborate on this)


In the following the assumption of normal distributed difference scores is tested with the `shapiro.test()` function of the `{stats}` package.
```{r dependent-shapiro}
# Conduct shapiro wilk test
dep_shapiro <- shapiro.test(dep_data$post - dep_data$pre)

# Output shapiro wilk test
dep_shapiro
```
The Shapiro-Wilk test resulted in a non-signifikant result, $W$ = `r dep_shapiro$statistic`, $p$ = `r dep_shapiro$p.value`. Hence, the assumption of normality is fulfilled.



## Conducting the dependent $t$-test
In `R` the $t$-test for dependent samples can be conducted with the `t.test()` function of the built-in `{stats}` package, specifying the argument `paired = TRUE`. In addition, the effect size Hedges' $g$ is calculated. In the context, of mean comparison Cohen's $d$ is a wiedly applied effect measure. However, Cohen's $d$ is positively biased. Therefore, Hedges' $g$ is used.

```{r dependent-t-test}
# Dependent t-test for data in wide format
dep_ttest <- t.test(dep_data$post, dep_data$pre,
                    alternative = "two.sided", # default; other: "less", "greater"
                    paired = TRUE)
# If the data is in long format use:
# t.test(post ~ pre, data = dep_ttest, paired = TRUE)

# Output
dep_ttest

# Calculate effect size hedges' g
# also possible to calculate (biased) cohen's d with cohens_d()
dep_effect <- hedges_g(dep_data$post, dep_data$pre, paired = TRUE)

# Output effect size
dep_effect
```

## Reporting the dependent $t$-test

On average, the pre-values ($M$ = `r dep_descr$numeric.mean[which(dep_descr$skim_variable == "pre")]`, $SD$ = `r dep_descr$numeric.sd[which(dep_descr$skim_variable == "pre")]`) were lower than the post-values ($M$ = `r dep_descr$numeric.mean[which(dep_descr$skim_variable == "post")]`, $SD$ = `r dep_descr$numeric.sd[which(dep_descr$skim_variable == "post")]`). The $t$-test for dependent samples resulted in a significant result, $t$(`r dep_ttest$parameter`) = `r dep_ttest$statistic`, $p$ = `r dep_ttest$p.value`, $g$ = `r dep_effect$Hedges_g`, 95%CI [`r dep_effect$CI_low`, `r dep_effect$CI_high`], indicating that the difference ($M$ = `r dep_descr$numeric.mean[which(dep_descr$skim_variable == "difference")]`, $SD$ = `r dep_descr$numeric.sd[which(dep_descr$skim_variable == "difference")]`) is significant.


## Visualizing Results

# Appendix: Theory and Mathematics of the $t$-tests


# Comparing an observed mean to an expected mean

# Comparing two independent groups

# Comparing two dependent groups






















