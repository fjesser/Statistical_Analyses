---
title: "Correlation"
author: "Felix EÃŸer"
date: "`r Sys.Date()`"
output: 
    html_document:
      toc: true
      toc_float: true
      number_sections: true
bibliography: '`r file.path(gsub("(?<=Statistical_Analyses).*", "", getwd(), perl = TRUE), "bibliography.bib")`'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction and Overview

Correlation coefficients quantify the degreee of covariation between two variables. Due to the possible combinations of different scales of measurement, several correlation coefficients exist. The presentation of the different correlation coefficients is structured along the different combinations of scales of measurement. 

For each correlation coefficient the general outline is as follows:
1. Assumptions of the correlation coefficient
2. Computation
3. Interpretation
4. Properties
5. Visualization

In order to have clear overview over the several correlation coefficients, the mathematical background is located in the appendix. 


# Load Packages and Data

```{r packages, message=FALSE}
library(tidyverse) # data wrangling, etc. 
```



```{r data}
glimpse(mtcars)
```



# Two Nominal Variables

## Two Dichotomous Variables

### The $\varphi$ coefficient - Product-Moment-Correlation of dichotomous variables

value range: [-1, +1]
boundary value can only be obtained if relationship is perfect **and** both variables have the same distribution.


$$\hat{\varphi} = \frac{n_{11}\cdot n_{22} - n_{12} \cdot n_{21}}{\sqrt{(n_{11} + n_{21}) \cdot (n_{12} + n_{22}) \cdot (n_{11} + n_{12}) \cdot (n_{21} + n_{22})}}$$

The more both distributions differ, the smaller the possible absolute value of $\varphi$.


There are corrections but it is easier to use other correlation measures for dichotomous variables.

### Yules Q

```{r}
library(correlation)

```


is $\hat{\gamma}$ for two dichotomous variables
is also a function of Odds Ratio (link to Odds ratio) that is normed to fall between -1 and 1

$$Q = \frac{n_{11}\cdot n_{22} - n_{12} \cdot n_{21}}{n_{11}\cdot n_{22} + n_{12} \cdot n_{21}}$$

```{r}
correlation(mtcars[, c("vs", "am")], method = "gamma")


```


```{r}
YuleCor(mtcars[, c("vs", "am")])
```



### Odds-Ratio

### Tetrachoric Correlation

Application of the polychoric correlation on dichotomous variables.


## Two Polytomous Variables

### Contingency Coefficient

Based on $\chi^2$ value. 
inferior to Cramer's $V$ becauseit can never reach it's upper boundary!

### Cramer's $V$

Based on the $\chi^2$ value
is identical to $\varphi$ in the case of dichotomous variables. 
value range: [0, 1]



## A Dichotomous and a Polytomous Variable

### Cramer's $V$


# Two Ordinal Variables

## Kendall's $\tau$
-> used for singular ordinal variables without tied ranks

## $e$ Coefficient
--> used for singular ordinal variables with tied ranks


## $\gamma$ Coefficient
-> used for ordinal variables with ordered categories


## Polychoric Correlation
-> used for ordinal variables with ordered categories
-> assumes that metric normally distributed variables underlie both ordinal variables with 

continuity correction
two-step estimation (quicker), ml estimation (slower and deprecated)

## Somer's $d$
-> used for a singular ordinal variable (can have ties) and a ordinal variable with ordered categories

# Two Metric Variables

## Pearson's Product-Moment-Correlation

Pearson's product-moment-correlation $r$ is a correlation coefficient between two metric variables. It is one of the most used and most popular correlation coefficient. 

Assumptions: 
- both variables have at least metric scale of measurement
- bivariate normal distribution of both variables
    + However, when testing $r$ against 0, the test is robust (Diehl & Arbinger, 2001) so that the assumption of bivariate normality is often not conducted.

Interpretation:
- The value range is: $[-1, +1]$
- Direction of the relationship
  - a negative sign means there is a negative relationship between the variables: Higher values on one variable go along with smaller values on the other variable.
  - a positive sign means there is a positive relationship between the variables: Higher values on one variable go along with higher values on the other variable. 
  - zero means there is no relationship between the variables
- Size of the relationship
    - A higher absolute value of $r$ indicates a stronger relationship.
    - What constitutes a weak or strong relationship depends on the application context.
    - However, [@Cohen1988]

Characteristics:
- As mentioned before, the value range of $r$ is: $[-1, +1]$
- $r$ is sensitive to outliers
    + Therefore, it is advisable to check for outliers (link to outliers or something like that)
- $r$ is a measure for the *linear* relationship between two variables
    + Thus, two variables can be (non-lineary) related, although $r = 0$.
    + Therefore, view the scatterplot before interpreting the $r$. 
- $r$ is invariant against linear transformations (link to something like transformations)

By means of two base `R` functions in the `{stats}` package, it is possible to calculate Pearson's $r$. The `cor()` function does not provide a statistical significance test, whereas `cor.test()` provides a significant test. In contrast to `cor.test()`, it is possible to create correlation matrices with the `cor()` function (see link to Correlation Matrices).

```{r pearson}
# Calculate Pearson correlation
cor_pearson <- cor.test(mtcars$mpg, mtcars$qsec, # default: method = "pearson"
                        alternative = "greater") # default: alternative = 
                                                          # "two.sided"; "less"

# Output Pearson correlation
cor_pearson
```

The correlation between the variables is $r$ (`r cor_pearson$parameter`) =`r cor_pearson$estimate`, $p =$ `r cor_pearson$p.value` (one-tailed). 

# A Nominal and a Ordinal Variable

# A Nominal and a Metric Variable

## A Dichotomous Nominal Variable and a Metric Variable

### Point-Biserial Correlation
### Biserial Correlation

## A Polytomous Nominal Variable and a Metric Variable
eta = square root of eta squared (from ANOVA)
-> eta for dichotomous nominal variable corresponds with point-biserial correlation


# An Ordinal and a Metric Variable

ordinal variable with ordered categories
polyserial correlation
- assumes that a metric variable underlies the ordinal variable


singular ordinal variable








# Appendix: Theory and Mathematics of the Correlation Coefficients




## Two Metric Variables

### Pearson's Product-Moment-Correlation

The first building block for calculating Pearson's product-moment correlation is the summed cross-product deviations of variable $X$ and variable $Y$:

$$CP_{XY} = \sum^n_{i = 1} (x_i - \bar{x}) \cdot (y_i - \bar{y})$$
The summed cross-product deviations is the sum of the products


# References
